---
layout: post
title: LLMs
excerpt: large language model tips and tricks
category: machine-learning
tags: [machine-learning]
---

## Calibration
A LLM is **highly calibrated** if it's predicted confidence in an answer generally matches the probability of being correct. Pre-trained GPT4 is highly calibrated. Post-training actually reduces its calibration.

For example, `P("Positive" | "Input: nothing Sentiment:")` should be 0.5 because there is no input given, so the sentiment is neutral. Yet GPT4 might state that the probability is higher, such as 0.9. This would mean that the LLM's **output distribution** is biased toward the label `Positive`.

To address this, we need to **calibrate** the output distribution so that a probability of 0.5 is assigned to both `Positive` and `Negative`.

## Why is calibration important?
As we integrate LLMs into systems and into society, we need to know their **uncertainty**. [2]

> When machine learning models are integrated into broader systems, it is critical for these models to be simultaneously accurate (i.e. frequently correct) and able to express their uncertainty (so that their errors can be appropriately anticipated and accommodated). Calibration and appropriate
expression of model uncertainty is especially critical for systems to be viable for deployment in high-stakes settings, including those where models inform decision-making (e.g. resume screening), which we increasingly see for language technology as its scope broadens. For example, if a model is uncertain in its prediction, a system designer could intervene by having a human perform the task instead to avoid a potential error (i.e. selective classification).



From page 10 of the GPT4 technical report, we know that GPT4 is biased and confidently wrong in its predictions because of the relatively poor calibration.

# Resources
* [[0] Calibrating LLMs](https://learnprompting.org/pt/docs/reliability/calibration)
* [[1] GPT4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
* [[2] Holistic Evaluation of Language Models by Stanford University](https://arxiv.org/pdf/2211.09110.pdf)