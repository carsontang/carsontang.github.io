---
layout: post
title: Transformers
excerpt: Intuitively understand what makes an ML transformer so powerful
category: machine-learning
tags: [machine-learning]
---

## Resources
* [The Positional Encoding by Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
* [Jay Alammmar's Guide to Transformers](https://jalammar.github.io/illustrated-transformer/#representing-the-order-of-the-sequence-using-positional-encoding)
* [Why add positional embedding instead of concatenate?](https://github.com/tensorflow/tensor2tensor/issues/1591)