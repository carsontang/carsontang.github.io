---
layout: post
title: LLMs
excerpt: large language model tips and tricks
category: machine-learning
tags: [machine-learning]
---

## Calibration
A LLM is **highly calibrated** if it's predicted confidence in an answer generally matches the probability of being correct. Pre-trained GPT4 is highly calibrated. Post-training actually reduces its calibration.

For example, `P("Positive" | "Input: nothing Sentiment:")` should be 0.5 because there is no input given, so the sentiment is neutral. Yet GPT4 might state that the probability is higher, such as 0.9. This would mean that the LLM's **output distribution** is biased toward the label `Positive`.

To address this, we need to **calibrate** the output distribution so that a probability of 0.5 is assigned to both `Positive` and `Negative`.

From page 10 of the GPT4 technical report, we know that GPT4 is biased and confidently wrong in its predictions because of the relatively poor calibration.

# Resources
* [Calibrating LLMs](https://learnprompting.org/pt/docs/reliability/calibration)
* [GPT4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)